{
  "name": "AI_Fact_Checker_Live",
  "nodes": [
    {
      "parameters": {
        "jsCode": "/**\n * Page_Splitter - Fixed to preserve Title from Add_Metadata\n */\nconst out = [];\n\nfor (const [itemIndex, item] of $input.all().entries()) {\n  const pages     = item.json.text;\n  const docId     = item.json.doc_id;\n  const ingestTs  = item.json.ingest_ts;\n  \n  // Get Title using itemMatching to reach back to Add_Metadata\n  let documentTitle = \"Unknown Document\";\n  try {\n    const originalMeta = $(\"Add_Metadata-Old\").itemMatching(itemIndex);\n    documentTitle = originalMeta?.Title || originalMeta?.document_title || \"Unknown Document\";\n    console.log(`Split Pages: Retrieved Title \"${documentTitle}\" from Add_Metadata`);\n  } catch (e) {\n    console.log('Could not access Add_Metadata:', e.message);\n    // Fallback to current item\n    documentTitle = item.metadata?.Title || item.json?.Title || item.json?.document_title || \"Unknown Document\";\n  }\n  \n  if (!Array.isArray(pages)) {\n    throw new Error(`Expected json.text to be an array but got ${typeof pages}`);\n  }\n  if (docId === undefined || ingestTs === undefined) {\n    throw new Error(\n      'Missing doc_id or ingest_ts on incoming item — ' +\n      'make sure Add_Metadata is directly upstream and has Duplicate Item ON'\n    );\n  }\n  \n  pages.forEach((pageText, idx) => {\n    out.push({\n      json: {\n        page:       idx + 1,\n        text:       pageText.trim(),\n        doc_id:     docId,\n        ingest_ts:  ingestTs,\n        Title:      documentTitle,  // Now properly preserved!\n        document_title: documentTitle  // Also set this for the Set node\n      },\n      pairedItem: {\n        item: itemIndex\n      }\n    });\n  });\n}\nreturn out;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3360,
        340
      ],
      "id": "a2d7d384-be2e-48ba-a7c3-4f9728c28ebd",
      "name": "Page_Splitter"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        -2600,
        280
      ],
      "id": "dcc8b1af-bb4c-4821-bbf3-2ee5423da26c",
      "name": "Merge_Text_Citations"
    },
    {
      "parameters": {
        "operation": "pdf",
        "binaryPropertyName": "data0",
        "options": {
          "joinPages": false
        }
      },
      "type": "n8n-nodes-base.extractFromFile",
      "typeVersion": 1,
      "position": [
        -4400,
        700
      ],
      "id": "07731b57-81b4-4c2b-8508-5efa4b986d98",
      "name": "PDF_To_Text",
      "disabled": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "49d5435e-949a-43d5-b092-51f30b85dded",
              "leftValue": "page",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -2820,
        280
      ],
      "id": "24791725-d32e-46df-91b9-33242437f9f4",
      "name": "IF_Page_Router"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * Citations_Dedupe\n * Input: items whose json.citations is an array of objects OR strings.\n * Output: same items, but citations[] contains only unique citation_id values.\n */\nconst seen = new Set();\n\nitem.json.citations = (item.json.citations || [])\n  .filter(cite => {\n    // normalise to object { citation_id, ... }\n    const id = typeof cite === 'string' ? cite : cite.citation_id;\n    if (!id) return false;\n    if (seen.has(id)) return false;\n    seen.add(id);\n    return true;\n  });\n\nreturn item;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2380,
        280
      ],
      "id": "e8ae7f33-a2db-4be8-8514-3802ddbbf6d5",
      "name": "Citations_Dedupe"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Normalise Perplexity response → unified schema\n */\nfunction normaliseVerdict(raw){\n  const v = (raw||'').toString().trim().toLowerCase();\n  if (['true','verified','supported'].includes(v))           return 'True';\n  if (['false','refuted','disputed'].includes(v))           return 'False';\n  if (['partially true','partial','mixed'].includes(v))     return 'Partially True';\n  return 'Unverifiable';\n}\n\nreturn items.map(itm => {\n  const j = itm.json;\n  return {\n    json:{\n      claim_id    : j.claim_id,\n      verdict     : normaliseVerdict(j.verdict),\n      confidence  : j.confidence,\n      explanation : j.explanation,\n      sources_used: Array.isArray(j.sources_used) ? j.sources_used : [],\n      doc_id      : j.doc_id,\n      ingest_ts   : j.ingest_ts,\n      processing_metadata:{\n        api_source :'Perplexity',\n        received_ts: new Date().toISOString(),\n        ...j.processing_metadata\n      }\n    }\n  };\n});\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        220,
        320
      ],
      "id": "82ec5da0-b203-4fbe-9a9b-e7263c3a4fee",
      "name": "Perplexity_Response_Parser"
    },
    {
      "parameters": {
        "jsCode": "const API_KEY = process.env.PERPLEXITY_API_KEY;\nconst MODEL_ID  = 'sonar-pro';\nconst BATCH     = 3;\nconst TIMEOUT   = 15_000;\n\nfunction normaliseVerdict(raw){\n  const v = (raw||'').toString().trim().toLowerCase();\n  if (['true','verified','supported'].includes(v)) return 'True';\n  if (['false','refuted','disputed'].includes(v)) return 'False';\n  if (['partially true','partial','mixed'].includes(v)) return 'Partially True';\n  return 'Unverifiable';\n}\n\nconst out = [];\nfor (const [i, item] of items.entries()) {\n  console.log(`🧪 DEBUG item #${i}:\\n`, JSON.stringify(item, null, 2));\n}\n\nfor (let i = 0; i < items.length; i += BATCH){\n  const slice = items.slice(i,i+BATCH);\n\n  const reqs = slice.map(item => (async () => {\n    const j = item.json;\n    const { claim_id, claim_text, source_paragraph, linked_citations = [], doc_id, ingest_ts } = j;\n\n    if (!claim_text || typeof claim_text !== 'string') {\n      return {\n        json: {\n          claim_id: claim_id || 'UNKNOWN',\n          verdict: 'Unverifiable',\n          confidence: 0,\n          explanation: 'Invalid or missing claim_text input.',\n          sources_used: [],\n          perplexity_sources: [],\n          processing_metadata: {\n            api_source: 'Perplexity',\n            received_ts: new Date().toISOString(),\n            error: true,\n            error_message: 'Missing or non-string claim_text'\n          }\n        }\n      };\n    }\n\n    console.log(`[Perplexity Helper] Validating claim_id ${claim_id}:\\n${claim_text}`);\n\n    // UPDATED: Better prompt that explicitly requests sources with URLs\n    const body = {\n      model: MODEL_ID,\n      temperature: 0,\n      max_tokens: 512,\n      return_citations: true,  // REQUEST CITATIONS\n      return_images: false,\n      messages: [\n        {\n          role: 'system',\n          content: `You are a fact-checking assistant. You MUST output ONLY valid JSON with these exact keys: verdict, confidence, explanation, sources_used.\n\nThe sources_used field MUST be an array of actual URLs you consulted to verify the claim. If you cannot find sources, use an empty array [].\n\nExample output:\n{\n  \"verdict\": \"True\",\n  \"confidence\": 85,\n  \"explanation\": \"Brief explanation of your findings\",\n  \"sources_used\": [\"https://example.com/article1\", \"https://news.com/story2\"]\n}`\n        },\n        {\n          role: 'user',\n          content: `Please fact-check this claim and provide the URLs of sources you consulted:\n\nCLAIM: \"${claim_text}\"\n\nCONTEXT: ${source_paragraph || 'No additional context provided.'}\n\nRemember: Include the actual URLs of sources you used in the sources_used array.`\n        }\n      ]\n    };\n\n    try {\n      const resp = await this.helpers.httpRequest({\n        method: 'POST',\n        url: 'https://api.perplexity.ai/chat/completions',\n        timeout: TIMEOUT,\n        headers: {\n          Authorization: `Bearer ${API_KEY}`,\n          'Content-Type': 'application/json'\n        },\n        body,\n        json: true\n      });\n\n      const raw = resp.choices?.[0]?.message?.content?.trim() ?? '';\n      \n      // UPDATED: Extract citations from Perplexity response\n      const citations = resp.citations || [];\n      console.log(`[Perplexity] Raw citations from API:`, citations);\n      \n      let parsed;\n      let apiSources = [];\n\n      try {\n        parsed = JSON.parse(raw);\n        console.log(`[Perplexity] Parsed JSON response:`, parsed);\n      } catch {\n        parsed = {\n          verdict: null,\n          confidence: 0,\n          explanation: raw,\n          sources_used: []\n        };\n      }\n\n      // UPDATED: Multiple ways to extract sources\n      // 1. From API citations array\n      if (Array.isArray(citations)) {\n        apiSources = citations\n          .filter(cite => cite && typeof cite === 'string' && cite.startsWith('http'))\n          .map(url => url.trim());\n      }\n      \n      // 2. From parsed JSON sources_used\n      if (Array.isArray(parsed.sources_used)) {\n        const jsonSources = parsed.sources_used\n          .filter(source => typeof source === 'string' && source.startsWith('http'))\n          .map(url => url.trim());\n        apiSources.push(...jsonSources);\n      }\n      \n      // 3. Extract URLs from the explanation text itself\n      const urlRegex = /https?:\\/\\/[^\\s\\)\\]\\,\\;\\\"\\'\\n]+/g;\n      const extractedUrls = (parsed.explanation || '').match(urlRegex) || [];\n      const cleanExtracted = extractedUrls.map(url => url.replace(/[,.\\)]+$/, '').trim());\n      apiSources.push(...cleanExtracted);\n      \n      // 4. Fallback to linked_citations from the document\n      if (apiSources.length === 0) {\n        apiSources = linked_citations.filter(cite => \n          typeof cite === 'string' && cite.startsWith('http')\n        );\n      }\n\n      // Dedupe and clean final sources\n      const finalSources = [...new Set(apiSources)].filter(url => \n        url && url.length > 10 && url.startsWith('http')\n      );\n\n      console.log(`[Perplexity] Claim ${claim_id}: Found ${finalSources.length} sources total`);\n      console.log(`[Perplexity] Sources:`, finalSources);\n\n      return {\n        json: {\n          claim_id,\n          verdict: normaliseVerdict(parsed.verdict),\n          confidence: parsed.confidence ?? 0,\n          explanation: parsed.explanation ?? raw,\n          sources_used: finalSources,\n          perplexity_sources: finalSources,\n          doc_id,\n          ingest_ts,\n          processing_metadata: {\n            api_source: 'Perplexity',\n            received_ts: new Date().toISOString(),\n            error: false,\n            citations_found: finalSources.length\n          }\n        }\n      };\n    } catch (err) {\n      console.error(`[Perplexity] Error for claim ${claim_id}:`, err.message);\n      return {\n        json: {\n          claim_id,\n          verdict: 'Unverifiable',\n          confidence: 0,\n          explanation: `API Error: ${err.message}`,\n          sources_used: [],\n          perplexity_sources: [],\n          doc_id,\n          ingest_ts,\n          processing_metadata: {\n            api_source: 'Perplexity',\n            received_ts: new Date().toISOString(),\n            error: true,\n            error_message: err.message\n          }\n        }\n      };\n    }\n  })());\n\n  out.push(...await Promise.all(reqs));\n}\n\nreturn out;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        60,
        280
      ],
      "id": "5979fd66-5898-4047-9022-758a66c0c8a3",
      "name": "Perplexity_Helper"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "loose",
            "version": 2
          },
          "conditions": [
            {
              "id": "0eea69de-2350-4962-b0ea-10abe39d83a9",
              "leftValue": "={{ $json.needs_tiebreaker === true }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "looseTypeValidation": true,
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        1460,
        400
      ],
      "id": "d8e26b23-7729-449e-91a1-1b4c149e6c61",
      "name": "Conflict_Check"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Flatten_Claims\n * --------------\n * - Accepts GPT extractor output (array of claim objects).\n * - Preserves every field from the extractor, especially `source_paragraph`.\n * - Adds Title, doc metadata, and page-level URLs.\n */\n\nconst EXTRA_META = ['doc_id', 'ingest_ts', 'Title'];   // copied forward\n\n/* Harvest page-level URL list that `URL_Extractor` produced earlier */\nconst urlsByPage = {};\ntry {\n  $('Document_URL_Collector').all().forEach(it => {\n    urlsByPage[it.json.page] = it.json.extracted_urls || [];\n  });\n} catch { /* safe ignore */ }\n\nconst out = [];\n\n$input.all().forEach((item, idx) => {\n  let raw = (item.json?.message?.content || '').toString().trim()\n              .replace(/^```(?:json)?/i, '').replace(/```$/, '');\n  if (!raw) return;\n\n  let claims;\n  try { claims = JSON.parse(raw); } catch {\n    console.error('[Flatten_Claims] JSON.parse failed on item', idx);\n    return;\n  }\n\n  /* emit one item per claim */\n  claims.forEach(claim => {\n    if (!claim.claim_id || !claim.claim_text) return;\n\n    /* copy metadata */\n    EXTRA_META.forEach(k => {\n      if (item.json[k]) claim[k] = item.json[k];\n    });\n\n    /* insure Title is always present */\n    claim.Title = claim.Title || item.json.Title || 'Unknown Document';\n\n    /* attach page-level URLs */\n    const p = claim.document_position?.page;\n    claim.extracted_urls = urlsByPage[p] || [];\n\n    out.push({ json: claim });\n  });\n});\n\nreturn out;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1560,
        280
      ],
      "id": "83064d15-5b63-4d28-b58d-3e678b6e8545",
      "name": "Flatten_Claims"
    },
    {
      "parameters": {
        "mode": "combine",
        "advanced": true,
        "mergeByFields": {
          "values": [
            {
              "field1": "document_position.page",
              "field2": "page"
            }
          ]
        },
        "joinMode": "enrichInput1",
        "options": {
          "clashHandling": {
            "values": {
              "resolveClash": "preferInput1",
              "mergeMode": "shallowMerge"
            }
          },
          "multipleMatches": "first"
        }
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        -1360,
        340
      ],
      "id": "e77fcab6-ea97-46bf-8750-b73a73958796",
      "name": "Merge"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4o-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        560,
        260
      ],
      "id": "9dea0e36-dd60-4c97-bcd6-e5a074e8ea48",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "SCetqv74jnS0O0Ml",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "sessionIdType": "customKey",
        "sessionKey": "FACTCHECK_SESSION_001"
      },
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "typeVersion": 1.3,
      "position": [
        700,
        260
      ],
      "id": "430da83f-e2a1-4f23-9cf2-65b210768b49",
      "name": "Simple Memory"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.toolSerpApi",
      "typeVersion": 1,
      "position": [
        820,
        260
      ],
      "id": "546a4bc3-419e-4774-8d8f-31c487ec257f",
      "name": "SerpAPI",
      "credentials": {
        "serpApi": {
          "id": "hrqlkfcKo3zLUl4k",
          "name": "SerpAPI account"
        }
      }
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "==You are a FACT-CHECKING_AGENT with one SerpAPI search available.\n\n**TASK**  \nVerify the claim below using exactly **one** web search.\n\n**CLAIM**  \n{{ $json.claim_text }}\n\n**CONTEXT**  \n{{ $json.source_paragraph }}\n\n**INSTRUCTIONS**  \n1. Perform up to 3 SerpAPI search using the most relevant key terms.  \n2. Examine the first page of results only.  \n3. Decide the claim’s accuracy.  \n4. Output **only** JSON—no prose, no markdown—following the schema.\n\n**OUTPUT SCHEMA**\n```json\n{\n  \"claim_id\": \"{{ $json.claim_id }}\",\n  \"verdict\": \"True|False|Partially True|Unverifiable\",\n  \"confidence\": 88,\n  \"explanation\": \"One 1–2-sentence justification citing evidence.\",\n  \"sources_used\": return actual URL (e.g., https:cnn.com)\n}\n",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2,
      "position": [
        600,
        40
      ],
      "id": "27de60d8-8d7f-4a2a-a4c3-27b31674b4c1",
      "name": "SerpAPI_Fact_Agent"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Check for conflicts in already-merged verification results\n * ---------------------------------------------------------------------------\n * Verdict enum:  True | False | Partially True | Unverifiable\n * Claude is required when verdicts differ OR confidence gap > 30.\n */\n\nconst VERDICT_SET = new Set(['True','False','Partially True','Unverifiable']);\nconst GAP_LIMIT   = 30;  // confidence difference that triggers tie-break\n\n/* helper to coerce any input to the enum */\nfunction normal(v){\n  v = (v||'').toString().trim();\n  if (VERDICT_SET.has(v)) return v;\n  return 'Unverifiable';\n}\n\n/* Use the already-merged data from Merge_All_Verifications */\nconst inputItems = $input.all();\n\nconsole.log(`[Conflicts] Processing ${inputItems.length} already-merged items`);\n\nconst processed = inputItems.map(item => {\n  const j = item.json;\n  const id = j.claim_id;\n  \n  // Extract verdicts from the merged data\n  const pVerd = normal(j.perplexity_verdict);\n  const sVerd = normal(j.serpapi_verdict);\n  const pConf = j.perplexity_confidence ?? 0;\n  const sConf = j.serpapi_confidence ?? 0;\n\n  const conflict = pVerd !== sVerd ||\n                   Math.abs(pConf - sConf) > GAP_LIMIT;\n\n  console.log(`[Conflicts] Claim ${id}: P=${pVerd}(${pConf}%) S=${sVerd}(${sConf}%) Conflict=${conflict}`);\n\n  return {\n    json: {\n      ...j,  // Keep all the existing merged data\n      \n      /* conflict bookkeeping */\n      has_conflict: conflict,\n      needs_tiebreaker: conflict && pVerd !== 'Unverifiable' && sVerd !== 'Unverifiable',\n      conflict_type: conflict\n                        ? (pVerd !== sVerd ? 'verdict_mismatch' : 'confidence_gap')\n                        : 'no_conflict',\n      processing_status: 'post_conflict_check'\n    }\n  };\n});\n\nconsole.log(`[Conflicts] Total=${inputItems.length}, WithConflict=${processed.filter(x=>x.json.has_conflict).length}`);\nreturn processed;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1240,
        400
      ],
      "id": "66dcdd98-53c3-40e4-bb7a-03f4ede67882",
      "name": "Comprehensive_Conflicts_Check"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.resend.com/emails",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Bearer re_F18u6hve_HBnEkUUjbahjCxK9JrJVYn5L"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "contentType": "raw",
        "rawContentType": "application/json",
        "body": "={{ $json.resend_body }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        3060,
        580
      ],
      "id": "0be0b511-c77f-4809-9578-d7b9d390bddc",
      "name": "HTTP Request"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "==You are CLAUDE-ARBITER, an expert fact-checking adjudicator.\n\n**TASK**  \nResolve disagreements between two fact checkers and produce a final verdict.\n\n**INPUT**  \nClaim ID … {{ $json.claim_id }}  \nClaim     … {{ $json.claim_text }}  \n\n‣ Perplexity → Verdict: {{ $json.perplexity_verdict }} ({{ $json.perplexity_confidence }}%)  \n Explanation: {{ $json.perplexity_explanation }}  \n\n‣ SerpAPI    → Verdict: {{ $json.serpapi_verdict }} ({{ $json.serpapi_confidence }}%)  \n Explanation: {{ $json.serpapi_explanation }}  \n\nSource context: {{ $json.source_paragraph }}\n\n**TOOLS**  \nYou may do ONE web search if you truly need new evidence.\n\n**OUTPUT – return *only* this JSON**  \n```json\n{\n  \"claim_id\": \"{{ $json.claim_id }}\",\n  \"final_verdict\": \"True|False|Partially True|Unverifiable\",\n  \"final_confidence\": 90,\n  \"tiebreaker_explanation\": \"Concise rationale citing evidence.\",\n  \"decision_factors\": [\"point 1\", \"point 2\", \"point 3\"],\n  \"sources_considered\": [\"https://…\", \"https://…\"],\n  \"resolution_method\": \"claude_tiebreaker\"\n}\n",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2,
      "position": [
        2120,
        100
      ],
      "id": "d11508d2-8764-495d-ab27-35dc8296cb46",
      "name": "Tie-Breaker_Agent"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Final_Verdict_Assembly v5 - WITH COMPLETE SOURCE PRESERVATION\n * -------------------------\n * Chooses the authoritative verdict and PRESERVES ALL SOURCES\n */\n\nconsole.log('=== FINAL VERDICT ASSEMBLY ===');\n\nreturn $input.all().map(item => {\n  const j = item.json;\n  let verdict, confidence, method, finalSources = [];\n  let explanation = '';\n\n  console.log(`Processing claim ${j.claim_id}:`);\n  console.log(`- tiebreaker_verdict: ${j.tiebreaker_verdict}`);\n  console.log(`- source_verdict: ${j.source_verdict}`);\n  console.log(`- serpapi_verdict: ${j.serpapi_verdict}`);\n  console.log(`- perplexity_verdict: ${j.perplexity_verdict}`);\n  console.log(`- all_sources_used: ${j.all_sources_used?.length || 0}`);\n  console.log(`- perplexity_sources: ${j.perplexity_sources?.length || 0}`);\n  console.log(`- serpapi_sources: ${j.serpapi_sources?.length || 0}`);\n\n  /* 1 ─ Tie-Breaker wins (highest priority) */\n  if (j.needs_tiebreaker && j.tiebreaker_verdict) {\n    verdict       = j.tiebreaker_verdict;\n    confidence    = j.tiebreaker_confidence || 85;\n    method        = 'claude_tiebreaker';\n    finalSources  = Array.isArray(j.sources_considered) ? j.sources_considered : [];\n    explanation   = j.tiebreaker_explanation || 'Resolved via expert analysis.';\n\n  /* 2 ─ Source Validator verified via in-document sources */\n  } else if (j.source_verdict === 'Supported') {\n    verdict       = 'True';\n    confidence    = 100;\n    method        = 'source_validator';\n    finalSources  = Array.isArray(j.checked_urls) ? j.checked_urls : [];\n    explanation   = j.explanation || 'Verified via in-document source(s).';\n\n  } else if (j.source_verdict === 'Weakly Supported') {\n    verdict       = 'Partially True';\n    confidence    = 75;\n    method        = 'source_validator';\n    finalSources  = Array.isArray(j.checked_urls) ? j.checked_urls : [];\n    explanation   = j.explanation || 'Partially supported by in-document source(s).';\n\n  } else if (j.source_verdict === 'Refuted') {\n    verdict       = 'False';\n    confidence    = 90;\n    method        = 'source_validator';\n    finalSources  = Array.isArray(j.checked_urls) ? j.checked_urls : [];\n    explanation   = j.explanation || 'Refuted by in-document source(s).';\n\n  /* 3 ─ SerpAPI fallback (for claims Perplexity couldn't verify) */\n  } else if (j.serpapi_verdict && j.serpapi_verdict !== 'Unverifiable') {\n    verdict       = j.serpapi_verdict;\n    confidence    = j.serpapi_confidence || 0;\n    method        = 'serpapi_fallback';\n    finalSources  = Array.isArray(j.serpapi_sources) ? j.serpapi_sources : [];\n    explanation   = j.serpapi_explanation || 'Verified via search results.';\n\n  /* 4 ─ Perplexity primary web verification */\n  } else if (j.perplexity_verdict && j.perplexity_verdict !== 'Unverifiable') {\n    verdict       = j.perplexity_verdict;\n    confidence    = j.perplexity_confidence || 0;\n    method        = 'perplexity_primary';\n    finalSources  = Array.isArray(j.perplexity_sources) ? j.perplexity_sources : [];\n    explanation   = j.perplexity_explanation || 'Verified via web research.';\n\n  /* 5 ─ Nothing could verify it */\n  } else {\n    verdict       = 'Unverifiable';\n    confidence    = 0;\n    method        = 'no_valid_results';\n    explanation   = 'Could not verify claim through available sources.';\n  }\n\n  // Clean bracketed numbers from explanation\n  if (explanation) {\n    explanation = explanation.replace(/\\[\\d+\\]/g, '').trim();\n  }\n\n  console.log(`→ Final verdict for claim ${j.claim_id}: ${verdict} (${method}) with ${finalSources.length} sources`);\n\n  return {\n    json: {\n      ...j,\n      final_verdict: verdict,\n      final_confidence: confidence,\n      resolution_method: method,\n      final_sources: finalSources,\n      explanation: explanation,\n      \n      // PRESERVE ALL SOURCES FOR HTML REPORT\n      serpapi_sources: j.serpapi_sources || [],\n      perplexity_sources: j.perplexity_sources || [],\n      checked_urls: j.checked_urls || [],\n      all_sources_used: j.all_sources_used || [],\n      \n      // Preserve unsupported sources\n      unsupported_urls: j.source_verdict === 'Unsupported' ? (j.checked_urls || []) : []\n    }\n  };\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1800,
        500
      ],
      "id": "c654c20a-9236-48c7-9e59-2d9c46a4e951",
      "name": "Final_Verdict_Assembly"
    },
    {
      "parameters": {
        "jsCode": "// Convert Report to Binary File\nconst item = $input.first();\nconst reportContent = item.json.report_content;\nconst filename = item.json.filename || 'fact_check_report.md';\n\n// Convert text to binary data\nconst binaryData = Buffer.from(reportContent, 'utf8').toString('base64');\n\nreturn {\n  json: {\n    ...item.json,\n    attachment_ready: true\n  },\n  binary: {\n    data: {\n      data: binaryData,\n      mimeType: 'text/markdown',\n      fileName: filename,\n      fileSize: reportContent.length\n    }\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2740,
        580
      ],
      "id": "cc5c5134-e946-438c-8928-e2a59148d1d2",
      "name": "Extract_from_File"
    },
    {
      "parameters": {
        "jsCode": "// HTML Fact-Check Report Generator\nconst allResults = $input.all();\nconst reportDate = new Date().toLocaleDateString('en-US'); // MM/DD/YYYY format\n\n// Extract basic info and clean title\nconst rawTitle = allResults[0]?.json?.Title?.replace(/^=/, '') || allResults[0]?.json?.doc_id || 'Unknown Document';\nconst docTitle = rawTitle.replace(/\\n/g, '').trim(); // Clean newline characters\nconst totalClaims = allResults.length;\n\n// Count final verdicts\nconst verdictCounts = {};\nallResults.forEach(item => {\n  const verdict = item.json.final_verdict || 'Unknown';\n  verdictCounts[verdict] = (verdictCounts[verdict] || 0) + 1;\n});\n\n// Sort results by verdict priority: True, Partially True, False, Unverifiable\nconst verdictOrder = {'True': 1, 'Partially True': 2, 'False': 3, 'Unverifiable': 4};\nconst sortedResults = [...allResults].sort((a, b) => {\n  const aVerdict = a.json.final_verdict || 'Unverifiable';\n  const bVerdict = b.json.final_verdict || 'Unverifiable';\n  return (verdictOrder[aVerdict] || 999) - (verdictOrder[bVerdict] || 999);\n});\n\n// Generate clean HTML\nconst htmlReport = `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"utf-8\">\n    <title>Fact Check Analysis & Source Verification</title>\n    <style>\n        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f1f5f9; padding: 20px; }\n        .container { max-width: 940px; margin: auto; background: #fff; border-radius: 14px; box-shadow: 0 6px 18px rgba(0,0,0,.08); }\n        .header { background: #2c3e50; color: #fff; text-align: center; padding: 20px; border-radius: 14px 14px 0 0; }\n        .header h1 { font-size: 2.5em; margin-bottom: 10px; font-weight: 300; }\n        .header .meta { opacity: 0.9; font-size: 1.1em; }\n        \n        .summary { padding: 30px; background: #f8f9fa; border-bottom: 1px solid #eee; }\n        .summary h2 { color: #2c3e50; margin-bottom: 20px; font-size: 1.8em; }\n        \n        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }\n        .stat { background: white; padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 5px 15px rgba(0,0,0,0.08); border-left: 4px solid; }\n        .stat.true { border-left-color: #27ae60; }\n        .stat.false { border-left-color: #e74c3c; }\n        .stat.partially-true { border-left-color: #f39c12; }\n        .stat.unverifiable { border-left-color: #95a5a6; }\n        .stat-number { font-size: 2.5em; font-weight: bold; margin-bottom: 5px; }\n        .stat-label { color: #666; font-size: 0.9em; }\n        \n        .claims-section { padding: 30px; }\n        .claims-section h2 { color: #2c3e50; margin-bottom: 30px; font-size: 1.8em; }\n        \n        .claim { border: 1px solid #eee; border-radius: 10px; margin: 20px 0; overflow: hidden; transition: transform 0.2s, box-shadow 0.2s; }\n        .claim:hover { transform: translateY(-2px); box-shadow: 0 10px 25px rgba(0,0,0,0.1); }\n        .claim-header { background: #f8f9fa; padding: 15px 20px; border-bottom: 1px solid #eee; display: flex; justify-content: space-between; align-items: center; }\n        .claim-number { font-weight: bold; color: #2c3e50; }\n        .verdict { padding: 8px 16px; border-radius: 20px; font-weight: bold; font-size: 0.9em; }\n        .verdict-true { background: #d4edda; color: #155724; }\n        .verdict-false { background: #f8d7da; color: #721c24; }\n        .verdict-partially-true { background: #fff3cd; color: #856404; }\n        .verdict-unverifiable { background: #e2e3e5; color: #383d41; }\n        \n        .claim-text, .explanation { padding: 15px 20px; }\n        .claim-text { border-bottom: 1px solid #f0f0f0; }\n        .explanation { background: #fafafa; }\n        \n        .sources-section { padding: 30px; background: #f8f9fa; border-top: 1px solid #eee; }\n        .sources-section h2 { color: #2c3e50; margin-bottom: 20px; font-size: 1.8em; }\n        .source-category { margin: 20px 0; }\n        .source-category h3 { color: #495057; margin-bottom: 10px; }\n        .source-category p { margin: 10px 0; color: #666; }\n        .source-category ul { margin: 10px 0; padding-left: 20px; }\n        .source-category li { margin: 8px 0; }\n        \n        .methodology { background: #2c3e50; color: white; padding: 20px; text-align: center; border-radius: 0 0 14px 14px; }\n        .methodology h3 { margin-bottom: 15px; font-size: 1.5em; }\n        .methodology p { opacity: 0.9; max-width: 600px; margin: 0 auto; }\n        \n        @media (max-width: 768px) {\n            .claim-header { flex-direction: column; align-items: stretch; text-align: center; }\n            .container { margin: 10px; border-radius: 10px; }\n            body { padding: 10px; }\n            .header h1 { font-size: 2em; }\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1>Fact Check Analysis & Source Verification</h1>\n            <div class=\"meta\">\n                <strong>${docTitle}</strong><br>\n                Generated: ${reportDate} • ${totalClaims} Claims Analyzed\n            </div>\n        </div>\n        \n        <div class=\"summary\">\n            <h2>Summary</h2>\n            <div class=\"stats\">\n                <div class=\"stat true\">\n                    <div class=\"stat-number\" style=\"color: #27ae60;\">${verdictCounts['True'] || 0}</div>\n                    <div class=\"stat-label\">True (${((verdictCounts['True'] || 0) / totalClaims * 100).toFixed(1)}%)</div>\n                </div>\n                <div class=\"stat partially-true\">\n                    <div class=\"stat-number\" style=\"color: #f39c12;\">${(verdictCounts['Partially True'] || 0) + (verdictCounts['Partial'] || 0)}</div>\n                    <div class=\"stat-label\">Part True (${(((verdictCounts['Partially True'] || 0) + (verdictCounts['Partial'] || 0)) / totalClaims * 100).toFixed(1)}%)</div>\n                </div>\n                <div class=\"stat false\">\n                    <div class=\"stat-number\" style=\"color: #e74c3c;\">${verdictCounts['False'] || 0}</div>\n                    <div class=\"stat-label\">False (${((verdictCounts['False'] || 0) / totalClaims * 100).toFixed(1)}%)</div>\n                </div>\n                <div class=\"stat unverifiable\">\n                    <div class=\"stat-number\" style=\"color: #95a5a6;\">${verdictCounts['Unverifiable'] || 0}</div>\n                    <div class=\"stat-label\">Unverified (${((verdictCounts['Unverifiable'] || 0) / totalClaims * 100).toFixed(1)}%)</div>\n                </div>\n            </div>\n        </div>\n        \n        <div class=\"claims-section\">\n            <h2>Detailed Analysis</h2>\n            ${sortedResults.map((item, index) => {\n                const verdict = item.json.final_verdict || 'Unknown';\n                // Fix CSS class generation - ensure it matches our CSS classes\n                let verdictClass;\n                if (verdict === 'True') verdictClass = 'true';\n                else if (verdict === 'False') verdictClass = 'false';\n                else if (verdict === 'Partially True') verdictClass = 'partially-true';\n                else if (verdict === 'Unverifiable') verdictClass = 'unverifiable';\n                else verdictClass = 'unverifiable';\n                \n                // Add warning icon for Partially True\n                const displayVerdict = verdict === 'Partially True' ? '⚠️ Part True' : verdict;\n                const showAnalysis = verdict !== 'True';\n                \n                return `<div class=\"claim\">\n                    <div class=\"claim-header\">\n                        <span class=\"claim-number\">Claim ${index + 1}</span>\n                        <span class=\"verdict verdict-${verdictClass}\">${displayVerdict}</span>\n                    </div>\n                    <div class=\"claim-text\">\n                        <strong>Claim:</strong> ${item.json.claim_text || 'Unknown claim'}\n                    </div>\n                    ${showAnalysis ? `<div class=\"explanation\">\n                        <strong>Analysis:</strong> ${item.json.explanation || item.json.perplexity_explanation || item.json.serpapi_explanation || item.json.tiebreaker_explanation || 'Analysis not available'}\n                    </div>` : ''}\n                </div>`;\n            }).join('')}\n        </div>\n\n        <div class=\"sources-section\">\n            <h2>Source References</h2>\n            ${(() => {\n                // Collect all sources from all fact-checking services\n                const allFactCheckSources = new Set();\n                const unsupportedPdfSources = new Set();\n                \n                // Helper function to clean Google redirect URLs\n                function cleanGoogleRedirectUrl(url) {\n                    if (url.includes('google.com/url?q=')) {\n                        const match = url.match(/[?&]q=([^&]+)/);\n                        if (match) {\n                            return decodeURIComponent(match[1]);\n                        }\n                    }\n                    return url;\n                }\n                \n                allResults.forEach((item, i) => {\n                    // Get sources from final_sources\n                    const finalSources = item.json.final_sources || [];\n                    finalSources.forEach(url => {\n                        if (typeof url === 'string' && url.startsWith('http')) {\n                            let cleanUrl = cleanGoogleRedirectUrl(url);\n                            allFactCheckSources.add(cleanUrl);\n                        }\n                    });\n                    \n                    // Check for unsupported PDF sources in unsupported_urls field\n                    const unsupportedUrls = item.json.unsupported_urls || [];\n                    unsupportedUrls.forEach(url => {\n                        if (typeof url === 'string' && url.startsWith('http')) {\n                            let cleanUrl = cleanGoogleRedirectUrl(url);\n                            if (cleanUrl.toLowerCase().includes('.pdf')) {\n                                unsupportedPdfSources.add(cleanUrl);\n                            }\n                            allFactCheckSources.add(cleanUrl);\n                        }\n                    });\n                    \n                    // Look for SerpAPI sources in various possible fields\n                    const possibleSourceFields = [\n                        'serpapi_sources', 'perplexity_sources', 'tiebreaker_sources',\n                        'sources_used', 'external_sources', 'research_sources',\n                        'fact_check_sources', 'supporting_sources', 'checked_urls',\n                        'supporting_urls', 'sources_considered'\n                    ];\n                    \n                    possibleSourceFields.forEach(field => {\n                        if (item.json[field]) {\n                            if (Array.isArray(item.json[field])) {\n                                item.json[field].forEach(source => {\n                                    if (typeof source === 'string' && source.startsWith('http')) {\n                                        let cleanUrl = cleanGoogleRedirectUrl(source);\n                                        allFactCheckSources.add(cleanUrl);\n                                    }\n                                });\n                            } else if (typeof item.json[field] === 'string' && item.json[field].startsWith('http')) {\n                                let cleanUrl = cleanGoogleRedirectUrl(item.json[field]);\n                                allFactCheckSources.add(cleanUrl);\n                            }\n                        }\n                    });\n                });\n                \n                let sourcesHtml = '';\n                \n                // Show all fact-checking sources\n                if (allFactCheckSources.size > 0) {\n                    sourcesHtml += `<div class=\"source-category\">\n                        <p>Sources used during the fact-checking process:</p>\n                        <ul>\n                            ${Array.from(allFactCheckSources).sort().map(url => \n                                `<li><a href=\"${url}\" target=\"_blank\">${url}</a></li>`\n                            ).join('')}\n                        </ul>\n                    </div>`;\n                }\n                \n                // Highlight unsupported PDF sources\n                if (unsupportedPdfSources.size > 0) {\n                    sourcesHtml += `<div class=\"source-category\">\n                        <h3>⚠️ Unsupported PDF Sources</h3>\n                        <p style=\"color: #e74c3c; font-weight: bold;\">These PDF sources from the original document could not support the claims. Please review manually:</p>\n                        <ul>\n                            ${Array.from(unsupportedPdfSources).sort().map(url => \n                                `<li style=\"background: #fff3cd; padding: 8px; margin: 5px 0; border-left: 4px solid #f39c12;\">\n                                    <a href=\"${url}\" target=\"_blank\" style=\"color: #856404; font-weight: bold;\">${url}</a>\n                                    <br><em style=\"color: #856404;\">Manual verification recommended</em>\n                                </li>`\n                            ).join('')}\n                        </ul>\n                    </div>`;\n                }\n                \n                // If no sources found at all\n                if (allFactCheckSources.size === 0 && unsupportedPdfSources.size === 0) {\n                    sourcesHtml += `<div class=\"source-category\">\n                        <p>Analysis was conducted using established methodologies and available reference materials.</p>\n                    </div>`;\n                }\n                \n                return sourcesHtml;\n            })()}\n        </div>\n\n        <div class=\"methodology\">\n            <h3>Methodology</h3>\n            <p>This analysis was conducted using AI-powered fact-checkers including Perplexity AI and SerpAPI, with Claude AI providing tie-breaker analysis for conflicting results.</p>\n        </div>\n    </div>\n</body>\n</html>`;\n\nreturn [{ json: { report_content: htmlReport } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2500,
        400
      ],
      "id": "58a39112-65cc-4f41-a6b3-6432eb568e97",
      "name": "HTML_Fact_Check_Report"
    },
    {
      "parameters": {
        "jsCode": "/**\n * HTML_Email – send via Resend sandbox domain\n */\nconst j         = $input.first().json;\nconst filename  = j.filename || 'fact_check_report.html';\nconst lbl       = (j.title || j.doc_id || 'Fact-Check Report').replace(/\\n/g, '').trim();\nconst s         = j.summary  || {};\nconst when      = new Date().toLocaleDateString();\n\nconst plain = `\nFact-check complete!\n\nDocument : ${lbl}\nGenerated: ${when}\nTotal    : ${s.total_claims ?? 0}\n\nTrue            : ${s.true ?? 0}\nFalse           : ${s.false ?? 0}\nPartially True  : ${s.partial ?? 0}\nUnverifiable    : ${s.unverifiable ?? 0}\n\nOpen the attached HTML file to read details.\n`;\n\nconst html = `<div style=\"font-family:Segoe UI,Arial,sans-serif;\nmax-width:600px;margin:auto;padding:20px;background:#f8f9fa\">\n  <h2 style=\"text-align:center;background:#2c3e50;color:#fff;padding:15px;border-radius:8px\">\n    📊 Fact-Check Complete!\n  </h2>\n  <p><strong>Document:</strong> ${lbl}<br>\n     <strong>Generated:</strong> ${when}<br>\n     <strong>Total Claims:</strong> ${s.total_claims ?? 0}</p>\n  <p>\n    ✅ True ${s.true ?? 0}&nbsp;| \n    ❌ False ${s.false ?? 0}&nbsp;| \n    ⚠️ Partial ${s.partial ?? 0}&nbsp;| \n    ❓ Unverifiable ${s.unverifiable ?? 0}\n  </p>\n  <p style=\"background:#667eea;color:#fff;padding:12px;border-radius:6px;text-align:center\">\n    Report attached as <strong>${filename}</strong>\n  </p>\n  <p style=\"text-align:center;font-size:.8em;color:#666\">Powered by AI Fact-Checker</p>\n</div>`;\n\nconst payload = {\n  from : 'Fact_Checker_Report@resend.dev',              // sandbox sender\n  to   : ['shannon@themessinagroup.com'],\n  subject : `Fact-Check Report – ${lbl}`,\n  text    : plain.trim(),\n  html    : html,\n  attachments: [{\n    filename,\n    content : Buffer.from(j.report_content,'utf8').toString('base64')\n  }]\n};\n\nreturn [{ json:{ resend_body: JSON.stringify(payload, null, 2) } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2900,
        580
      ],
      "id": "5da90c5f-41f4-4294-ab7b-f739278ecca4",
      "name": "HTML_Email"
    },
    {
      "parameters": {
        "jsCode": "// Loop over input items and add a new field called 'myNewField' to the JSON of each one\nfor (const item of $input.all()) {\n  item.json.myNewField = 1;\n}\n\nreturn $input.all();"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3800,
        340
      ],
      "id": "b51b1140-d64c-455e-b0e2-22162d4ee8e0",
      "name": "Extract_Document_Title"
    },
    {
      "parameters": {
        "jsCode": "// Store original data including title\nconst items = $input.all();\n\nreturn items.map(item => ({\n  json: {\n    ...item.json,\n    original_claim_text: item.json.claim_text,\n    original_title: item.json.Title || \n                   item.json.title ||\n                   item.json.document_title ||\n                   item.json.filename ||\n                   null,\n  }\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1040,
        340
      ],
      "id": "93bae0a4-2e61-400c-8d8e-757ff0095387",
      "name": "Store_Original_Data"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "claude-3-7-sonnet-20250219",
          "cachedResultName": "Claude 3.7 Sonnet"
        },
        "options": {
          "maxTokensToSample": 1000,
          "temperature": 0.2
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "typeVersion": 1.3,
      "position": [
        2120,
        260
      ],
      "id": "a3d1d741-86b3-43a7-af36-2abfe2a4cf30",
      "name": "Claude",
      "credentials": {
        "anthropicApi": {
          "id": "kCa0r0rbt6NqatNA",
          "name": "Anthropic account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Restore Claim Text and Title from Store_Original_Data\nconst items = $input.all();\nconst originalData = $('Store_Original_Data').all();\n\nconsole.log('=== RESTORE CLAIM TEXT ===');\nconsole.log('Items to restore:', items.length);\nconsole.log('Original data available:', originalData.length);\n\nreturn items.map((item, index) => {\n  // Find matching original data by claim_id\n  const originalItem = originalData.find(orig => orig.json.claim_id === item.json.claim_id);\n  \n  if (originalItem) {\n    console.log(`Restoring claim ${item.json.claim_id}: \"${originalItem.json.original_claim_text}\"`);\n    \n    return {\n      json: {\n        ...item.json,\n        claim_text: originalItem.json.original_claim_text || item.json.claim_text || 'Unknown claim',\n        Title: originalItem.json.original_title || item.json.Title || 'Unknown Document'\n      }\n    };\n  } else {\n    console.log(`No original data found for claim ${item.json.claim_id}`);\n    return {\n      json: {\n        ...item.json,\n        claim_text: item.json.claim_text || 'Unknown claim',\n        Title: item.json.Title || 'Unknown Document'\n      }\n    };\n  }\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        0,
        0
      ],
      "id": "586f62cf-b526-492d-ad7f-ef8a34abd6a1",
      "name": "Restore_Claim_Text"
    },
    {
      "parameters": {
        "jsCode": "// ULTIMATE DEBUG - Check ALL possible sources for Title\nconst items = $input.all();\n\nconsole.log('=== ULTIMATE TITLE DEBUG ===');\nconsole.log('Input items count:', items.length);\nconsole.log('First input item keys:', Object.keys(items[0]?.json || {}));\n\nlet foundTitle = 'Unknown Document';\n\n// Check ALL nodes that might have the title\nconst nodesToCheck = ['Add_Metadata', 'Add Metadata', 'Metadata', 'Document_Input', 'PDF_Input', 'File_Input'];\n\nfor (const nodeName of nodesToCheck) {\n  try {\n    const nodeData = $(nodeName).all();\n    console.log(`--- ${nodeName} ---`);\n    console.log(`Count: ${nodeData.length}`);\n    if (nodeData.length > 0) {\n      console.log(`First item keys:`, Object.keys(nodeData[0]?.json || {}));\n      console.log(`Title in json:`, nodeData[0]?.json?.Title);\n      console.log(`Title in metadata:`, nodeData[0]?.metadata?.Title);\n      console.log(`document_title in json:`, nodeData[0]?.json?.document_title);\n      console.log(`Full json:`, JSON.stringify(nodeData[0]?.json, null, 2));\n      \n      // Actually capture the title if found\n      const title = nodeData[0]?.json?.Title || \n                   nodeData[0]?.metadata?.Title || \n                   nodeData[0]?.json?.document_title;\n      \n      if (title && title !== 'Unknown Document' && title.trim() !== '') {\n        foundTitle = title;\n        console.log(`✅ FOUND TITLE: \"${foundTitle}\" from ${nodeName}`);\n        break;\n      }\n    }\n  } catch (e) {\n    console.log(`${nodeName}: Not accessible`);\n  }\n}\n\nconsole.log(`Final title to use: \"${foundTitle}\"`);\n\n// Return items with the title actually set\nreturn items.map(item => ({\n  json: {\n    ...item.json,\n    Title: foundTitle,\n    DocumentTitle: foundTitle  // Set both for compatibility\n  }\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2320,
        500
      ],
      "id": "8b24bc96-620a-42d9-911c-5a51dcf99865",
      "name": "Restore_Title"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "1c645be3-81df-4b97-aec1-9d0305722c3c",
              "name": "=doc_id",
              "value": "=={{Date.now().toString()}}",
              "type": "string"
            },
            {
              "id": "e30d5e24-1504-4029-8212-88441046e5ab",
              "name": "=ingest_ts",
              "value": "=={{new Date().toISOString()}}",
              "type": "string"
            },
            {
              "id": "78de09f1-93a3-4d48-a092-6eb528339dfe",
              "name": "Title",
              "value": "={{ $('Webhook').first().binary.data0.fileName || \"No webhook filename\" }}\n",
              "type": "string"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -3620,
        340
      ],
      "id": "a8f1cc89-ca2b-4b27-8b62-9885c0cdd3e6",
      "name": "Add_Metadata"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4-turbo",
          "mode": "list",
          "cachedResultName": "GPT-4-TURBO"
        },
        "messages": {
          "values": [
            {
              "content": "=system\\nYou are an expert claim extractor … (rules)\\n\nuser\\n=={\\n \"doc_id\": \"{{ $json.doc_id }}\",\\n \"page\": {{ $json.page }},\\n \"citations\": {{ JSON.stringify($json.citations ?? []) }},\\n \"page_text\": \"<<<START>>>{{ $json.text }}<<<END>>>\"\\n}\\n\nNew schema (remove context_sources, keep linked_citations)\njson\\n[ {\\n \"claim_id\":\"…\",\\n \"claim_text\":\"…\",\\n \"source_paragraph\":\"…\",\\n \"linked_citations\":[],\\n \"document_position\":{\"page\":1,\"section\":\"first six words\"}\\n}]\\n",
              "role": "system"
            },
            {
              "content": "=={\n  \"doc_id\": \"{{ $json.doc_id }}\",\n  \"page\": {{ $json.page }},\n  \"citations\": {{ JSON.stringify($json.citations ?? []) }},\n  \"page_text\": \"<<<START>>>\\n{{ $json.text }}\\n<<<END>>>\"\n}"
            }
          ]
        },
        "options": {
          "temperature": 0,
          "topP": 1
        }
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        -1940,
        280
      ],
      "id": "819fad58-ac1a-4169-b635-a49e1e51754a",
      "name": "Claim_Extractor_GPT4o",
      "credentials": {
        "openAiApi": {
          "id": "SCetqv74jnS0O0Ml",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * Citation_Parser\n * — capture claim_id plus full citation context (footnotes, URLs, inline markers)\n */\nconst text     = item.json.text || '';\nconst page     = item.json.page;\nconst doc      = item.json.doc_id ?? '';\nconst claim_id = item.json.claim_id;\n\nconst citations = [];\nconst seen      = new Set();\n\n// 1) Footnote definitions (lines like \"5. https://…\" or \"5. Author, Title…\")\nconst lines = text.split(/\\r?\\n/);\nfor (const line of lines) {\n  // match \"N. <anything>\" up to end of line\n  const m = line.match(/^\\s*(\\d{1,3})\\.\\s*(.+\\S)\\s*$/);\n  if (!m) continue;\n  const num     = m[1];\n  const content = m[2].trim();\n  const id      = `p${page}_fn_${num}`;\n  if (seen.has(id)) continue;\n  seen.add(id);\n  citations.push({\n    claim_id,\n    citation_id: id,\n    raw:         content,  // full line after \"N.\"\n    number:      num,\n    page,\n    doc_id:      doc\n  });\n}\n\n// 2) Inline markers fallback: superscript, bracketed, or bare numbers\nconst markerRx = /\\[\\s*(\\d{1,3})\\s*\\]|\\(\\s*(\\d{1,3})\\s*\\)|([¹²³⁴⁵⁶⁷⁸⁹⁰])|b(\\d{1,3})\\b/g;\nconst SUPER    = { '¹':'1','²':'2','³':'3','⁴':'4','⁵':'5','⁶':'6','⁷':'7','⁸':'8','⁹':'9','⁰':'0' };\n\nfor (const m of text.matchAll(markerRx)) {\n  let num = m[1]||m[2]||m[3]||m[4]||'';\n  num = num.split('').map(ch => SUPER[ch]||ch).join('');\n  if (!/^\\d{1,3}$/.test(num)) continue;\n  const id = `p${page}_sup_${num}`;\n  if (seen.has(id)) continue;\n  seen.add(id);\n\n  // snippet around the marker as fallback\n  const idx   = m.index || 0;\n  const start = Math.max(0, idx - 40);\n  const end   = Math.min(text.length, idx + 40);\n  const snippet = text.slice(start, end).replace(/\\s+/g,' ').trim();\n\n  citations.push({\n    claim_id,\n    citation_id: id,\n    raw:         snippet,\n    number:      num,\n    page,\n    doc_id:      doc\n  });\n}\n\nitem.json.citations = citations;\nreturn item;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3040,
        280
      ],
      "id": "999208f1-f45d-4202-96e4-63d0c2267780",
      "name": "Citation_Parser"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Footnote_Map_Builder\n * --------------------\n * Build { number → url } for every citation and cache it on globalThis\n * so any later node in the SAME execution can read it.\n *\n * Place right after `Citations_Dedupe`.\n */\n\nconst map = {};\n\n// harvest every parsed citation\n$('Citations_Dedupe').all().forEach(it => {\n  (it.json.citations || []).forEach(c => {\n    const num = c.number;\n    if (!num) return;\n\n    // crude URL pull from raw foot-note line / inline link\n    const url = (c.raw.match(/https?:\\/\\/\\\\S+/) || [null])[0];\n    if (url) map[num] = url;\n  });\n});\n\n/* ----------  store on the execution object  ---------- */\nglobalThis.FOOTNOTE_MAP = map;      // lives only for this workflow run\n/* ------------------------------------------------------ */\n\nreturn $input.all();                // pass items unchanged\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2160,
        280
      ],
      "id": "d8b42e12-601d-44ef-b898-f17d4ae3df3b",
      "name": "Footnote_Map_Builder"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Attach_Supporting_URLs (Hybrid)\n * -------------------------------\n * Uses:\n *  - linked_citations + FOOTNOTE_MAP (primary)\n *  - page_urls, extracted_urls, citations (fallback)\n */\n\nconst foot = globalThis.FOOTNOTE_MAP || {};\n\nreturn $input.all().map(it => {\n  const linked = it.json.linked_citations || [];\n\n  // Step 1: Pull from citation map\n  const nums = linked\n     .map(id => id.match(/_(\\d{1,3})$/)?.[1])\n     .filter(Boolean);\n\n  const citationUrls = nums.map(n => foot[n]).filter(Boolean);\n\n  // Step 2: Fallbacks\n  const extracted = it.json.extracted_urls || [];\n  const page = it.json.page_urls || [];\n  const cited = it.json.citations || [];\n\n  // Step 3: Combine all sources and dedupe\n  const all = [...citationUrls, ...page, ...extracted, ...cited];\n  const deduped = [...new Set(all)];\n\n  it.json.supporting_urls = deduped;\n  return it;\n});\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -880,
        340
      ],
      "id": "1fa945d5-7962-4fcc-8082-c853c95b5568",
      "name": "Attach_Supporting_URLs"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "04d64373-6d92-473a-a454-340ffcc471d3",
              "leftValue": "={{ $json.source_verdict }}",
              "rightValue": "Unsupported",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            },
            {
              "id": "9e811547-3f5a-41ba-85b9-a31a0a3c5f66",
              "leftValue": "={{ $json.source_verdict }}",
              "rightValue": "Weakly Supported",
              "operator": {
                "type": "string",
                "operation": "equals",
                "name": "filter.operator.equals"
              }
            }
          ],
          "combinator": "or"
        },
        "options": {
          "ignoreCase": false
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -120,
        340
      ],
      "id": "ac308c35-87ee-477b-be41-3229aa7644b2",
      "name": "IF_Source_Unsupported"
    },
    {
      "parameters": {
        "jsCode": "// Transform Source_Validator output into the same shape Perplexity would create\nreturn $input.all().map(it => ({\n  json: {\n    ...it.json,\n    verdict      : 'True',\n    confidence   : 100,\n    explanation  : 'Verified via in-PDF source(s).',\n    sources_used : it.json.checked_urls\n  }\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        680,
        540
      ],
      "id": "76cccb6f-72f2-44aa-8df1-cbed519b5abb",
      "name": "Skip_Perplexity"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Document_URL_Collector\n * Extract EVERY url on the page—even wrapped links—and\n * save them to json.page_urls[] so later nodes can see them.\n */\nfunction scrape(txt) {\n  const joined = txt.replace(/https?:\\/\\/[^\\s]*\\n[^\\s]+/g,\n                             m => m.replace(/\\n/g, ''));\n  return [...new Set(Array.from(joined.matchAll(/https?:\\/\\/[^\\s)]+/g), m => m[0]))];\n}\nreturn $input.all().map(it => {\n  it.json.page_urls = scrape(it.json.text || '');\n  return it;\n});\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3260,
        560
      ],
      "id": "3ff905a4-981c-4e74-97bb-1ad06f6237be",
      "name": "Document_URL_Collector"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Extract_Hyperlinks - Handles both string and array input\n */\n\nconst item = $input.first();\nlet text = item.json.text;\n\nif (Array.isArray(text)) {\n  text = text.join('\\n'); // Merge array of page strings\n} else if (typeof text !== 'string') {\n  throw new Error(\"Extract_Hyperlinks: Expected text to be a string or array of strings.\");\n}\n\n// Extract URLs from text\nconst urlRegex = /https?:\\/\\/[^\\s\"'<>)\\]]+/g;\nconst urls = Array.from(new Set(text.match(urlRegex) || []));\n\nitem.json.urls = urls;\nitem.json.urlsFound = urls.length;\n\nconsole.log(`[Extract_Hyperlinks] Found ${urls.length} URLs in text input.`);\nreturn [item];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -4020,
        340
      ],
      "id": "a369ba85-8a38-40ed-a7c3-21a25c888da3",
      "name": "Extract_Hyperlinks_PDF"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "02187aa1-b852-469f-bf19-5eed43e92011",
        "responseMode": "responseNode",
        "options": {
          "binaryPropertyName": ""
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -4880,
        340
      ],
      "id": "3f1a8779-bd30-40de-9e22-d7f9486b6f9b",
      "name": "Webhook",
      "webhookId": "02187aa1-b852-469f-bf19-5eed43e92011"
    },
    {
      "parameters": {
        "jsCode": "const item = $input.first();\nconst bin = item.binary?.data0;\n\nif (!bin) {\n  throw new Error('❌ No file found in binary.data0');\n}\n\nif (!bin.data || typeof bin.data !== 'string') {\n  throw new Error('❌ binary.data0.data is not a base64 string');\n}\n\nif (!bin.mimeType?.toLowerCase().includes('pdf')) {\n  throw new Error(`❌ Expected PDF, got: ${bin.mimeType}`);\n}\n\nreturn [item];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -4540,
        340
      ],
      "id": "2a372e53-721a-4270-98eb-9562d7864809",
      "name": "Validate_PDF_Upload"
    },
    {
      "parameters": {
        "jsCode": "// Check if binary.data0 exists\nconst item = $input.first();\nconst binary = item.binary?.data0;\n\nif (!binary) {\n  throw new Error(\"❌ Missing binary.data0\");\n}\n\n// Handle raw Buffer (already in memory) or convert from filesystem-v2\nif (binary.data) {\n  // Already has base64 data? Do nothing\n  if (typeof binary.data === 'string') {\n    return [item];\n  }\n\n  // Otherwise, it's a Buffer (happens on self-hosted sometimes)\n  const base64 = Buffer.from(binary.data).toString('base64');\n  binary.data = base64;\n  return [item];\n}\n\nthrow new Error(\"❌ Binary object exists but no usable .data\");\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -4700,
        340
      ],
      "id": "a3bd6527-4816-4037-ab47-05889339a880",
      "name": "Move Binary Data",
      "executeOnce": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "9b836dae-eb7c-4a3f-b8fa-d0e1c4c15b45",
              "leftValue": "={{ $json.text && $json.text.length > 0 }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -4240,
        340
      ],
      "id": "4e4c1f31-4dcd-444b-b32c-f67f77e06cea",
      "name": "Has Text?"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Converts json.text (string) → array of pages\n * Preserves metadata fields\n */\n\nconst input = $input.first();\nlet text = input.json.text;\n\nif (Array.isArray(text)) {\n  return [input]; // Already valid\n}\nif (typeof text !== 'string') {\n  throw new Error(\"Text_To_Pages: Expected text to be a string or array.\");\n}\n\n// Naive split on double newlines or form feed\nconst pages = text.split(/\\f+|\\n{2,}/).map(p => p.trim()).filter(Boolean);\n\nreturn [{\n  json: {\n    ...input.json,\n    text: pages\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -3500,
        340
      ],
      "id": "5ebe8b2d-0789-495f-9c07-aeef55263bf7",
      "name": "Text_To_Pages"
    },
    {
      "parameters": {
        "sessionIdType": "customKey",
        "sessionKey": "FACTCHECK_SESSION_001"
      },
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "typeVersion": 1.3,
      "position": [
        -640,
        560
      ],
      "id": "ac3ec649-5333-46b7-b5b2-8e02353f4cd7",
      "name": "Simple Memory1"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4o-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        -760,
        520
      ],
      "id": "ce0b5ec2-ab4f-4bbb-b3a8-cab769c50262",
      "name": "OpenAI Chat Model1",
      "credentials": {
        "openAiApi": {
          "id": "SCetqv74jnS0O0Ml",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=You are a SOURCE_VALIDATOR_AGENT.\n\nYour job is to determine whether any of the URLs provided support or refute the given claim. SerpAPI_Search tool needs to  visit each URL and analyze its content to see if it supports a claim. \n\n---\n\n📄 CLAIM\n{{ $json.claim_text }}\n\n🌐 SUPPORTING_URLS\n{{ $json.supporting_urls }}\n\n🧠 INSTRUCTIONS\n\n1. Visit and review the content of each URL in `supporting_urls`.  \n   - You must rely solely on these URLs to evaluate the claim.  \n   - You may summarize key statements or evidence found at those URLs.\n\n2. Based on the content of the URLs, make a decision using the following verdicts:\n\n   - `Supported`: One or more URLs clearly and directly back the claim.\n   - `Weakly Supported`: The URLs suggest partial support, but the connection is indirect or vague.\n   - `Refuted`: One or more URLs contradict the claim.\n   - `Unsupported`: None of the URLs confirm or refute the claim.\n\n---\n\n🧾 OUTPUT FORMAT  \nRespond only in this JSON structure:\n\n```json\n{\n  \"claim_id\": \"{{ $json.claim_id }}\",\n  \"source_verdict\": \"Supported | Weakly Supported | Refuted | Unsupported\",\n  \"checked_urls\": [\"https://example1.com\", \"https://example2.com\"],\n  \"explanation\": \"1–2 sentence explanation justifying the verdict based on the content of the URLs.\"\n}\n",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2,
      "position": [
        -720,
        340
      ],
      "id": "76012af5-417e-4edb-9420-d7e7aa30a4d4",
      "name": "Source_Validator_Agent_V2"
    },
    {
      "parameters": {
        "jsCode": "return $input.all().map(item => {\n  const j = item.json;\n\n  return {\n    json: {\n      claim_id: j.claim_id ?? j.claim?.claim_id ?? 'UNKNOWN',\n      claim_text: j.claim_text ?? j.claim?.claim_text ?? 'MISSING CLAIM',\n      source_paragraph: j.source_paragraph ?? '',\n      ...j  // preserve the rest\n    }\n  };\n});\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1200,
        340
      ],
      "id": "2494655e-1f68-4230-b3b5-7cfd62300866",
      "name": "Fix_Claims_ID"
    },
    {
      "parameters": {
        "options": {
          "no_cache": "=[\n  {\n    \"name\": \"q\",\n    \"value\": \"={{ $json.claim_text }}\"\n  }\n]\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.toolSerpApi",
      "typeVersion": 1,
      "position": [
        -520,
        520
      ],
      "id": "d774b63b-8cd1-40a0-b761-03d14d5c4590",
      "name": "SerpAPI1",
      "credentials": {
        "serpApi": {
          "id": "hrqlkfcKo3zLUl4k",
          "name": "SerpAPI account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "return $input.all().map(item => {\n  let raw = item.json.output ?? '';\n  let parsed = {};\n\n  console.log(`[Parse_Validator] Raw output length: ${raw.length}`);\n\n  // More aggressive cleaning - remove markdown wrapper AND normalize line breaks\n  raw = raw\n    .replace(/^```json\\s*/i, '')  // Remove opening ```json\n    .replace(/\\s*```$/i, '')      // Remove closing ```\n    .replace(/\\\\n/g, '\\n')        // Convert literal \\n to actual newlines\n    .replace(/\\\\\"/g, '\"')         // Convert escaped quotes\n    .trim();\n\n  console.log(`[Parse_Validator] Cleaned output: ${raw}`);\n\n  try {\n    parsed = JSON.parse(raw);\n    console.log(`[Parse_Validator] Successfully parsed claim_id: ${parsed.claim_id}`);\n  } catch (e) {\n    console.error(`[Parse_Validator] JSON parse failed: ${e.message}`);\n    console.error(`[Parse_Validator] Failed on: ${raw.slice(0, 200)}...`);\n    \n    parsed = {\n      claim_id: item.json.claim_id || 'PARSE_ERROR',\n      source_verdict: 'ParseError',\n      explanation: `Parse failed: ${e.message}`\n    };\n  }\n\n  return {\n    json: {\n      ...item.json,\n      ...parsed\n    }\n  };\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -420,
        340
      ],
      "id": "2d2798b2-4e79-45e4-b5a2-01af9581b9b4",
      "name": "Parse_Validator_JSON"
    },
    {
      "parameters": {
        "jsCode": "return $input.all().map(item => {\n  const verdict = item.json.source_verdict;\n  const id = item.json.claim_id;\n  console.log(`🎯 IF Source Input → claim_id=${id} | verdict=${verdict}`);\n  return item;\n});\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -260,
        340
      ],
      "id": "f826897f-d1a9-4c18-ae8b-efcc998bbf52",
      "name": "Debug_Verification_Route"
    },
    {
      "parameters": {
        "jsCode": "// Merge all inputs by claim_id and preserve ALL verification data INCLUDING SOURCES\nlet inputs = [];\n\n// Collect inputs from different nodes - handle missing nodes gracefully\ntry {\n  const perplexityItems = $('Perplexity_Response_Parser').all();\n  inputs.push(...perplexityItems);\n  console.log(`Found ${perplexityItems.length} items from Perplexity_Response_Parser`);\n} catch (e) {\n  console.log('Perplexity_Response_Parser not found or empty');\n}\n\ntry {\n  const skipPerplexityItems = $('Skip_Perplexity').all();\n  inputs.push(...skipPerplexityItems);\n  console.log(`Found ${skipPerplexityItems.length} items from Skip_Perplexity`);\n} catch (e) {\n  console.log('Skip_Perplexity not found or empty');\n}\n\ntry {\n  const serpApiItems = $('SerpAPI_Response_Parser').all();\n  inputs.push(...serpApiItems);\n  console.log(`Found ${serpApiItems.length} items from SerpAPI_Response_Parser`);\n} catch (e) {\n  console.log('SerpAPI_Response_Parser not found, trying SerpAPI_Fact_Agent...');\n  try {\n    const serpApiDirectItems = $('SerpAPI_Fact_Agent').all();\n    inputs.push(...serpApiDirectItems);\n    console.log(`Found ${serpApiDirectItems.length} items from SerpAPI_Fact_Agent directly`);\n  } catch (e2) {\n    console.log('SerpAPI_Fact_Agent also not found or empty');\n  }\n}\n\nconsole.log('=== MERGE INPUT DEBUG ===');\nconsole.log(`Total inputs collected: ${inputs.length}`);\n\ninputs.forEach((item, i) => {\n  console.log(`Input ${i}:`, {\n    claim_id: item.json.claim_id,\n    verdict: item.json.verdict,\n    has_sources_used: !!item.json.sources_used,\n    has_source_verdict: !!item.json.source_verdict,\n    serpapi_sources: item.json.serpapi_sources,\n    perplexity_sources: item.json.perplexity_sources,\n    sources_count: (item.json.sources_used || []).length,\n    processing_metadata: item.json.processing_metadata?.api_source,\n    keys: Object.keys(item.json)\n  });\n});\n\nconst merged = {};\n\nfor (const item of inputs) {\n  let id = item.json.claim_id;\n  if (!id) {\n    console.log('Skipping item with no claim_id:', item.json);\n    continue;\n  }\n  if (typeof id === \"number\") id = id.toString();\n  if (typeof id === \"string\") id = id.trim();\n  \n  if (!merged[id]) {\n    merged[id] = {\n      claim_id: id,\n      verdicts: [],\n      claim_text: item.json.claim_text || null,\n      title: item.json.title || null,\n      // Initialize verification fields\n      source_verdict: null,\n      checked_urls: [],\n      perplexity_verdict: null,\n      perplexity_sources: [],\n      perplexity_explanation: null,\n      serpapi_verdict: null,\n      serpapi_sources: [],\n      serpapi_explanation: null,\n      // Preserve ALL sources\n      all_sources_used: []\n    };\n  }\n  \n  // Collect all sources from this item\n  const itemSources = [\n    ...(item.json.sources_used || []),\n    ...(item.json.serpapi_sources || []),\n    ...(item.json.perplexity_sources || []),\n    ...(item.json.checked_urls || [])\n  ].filter(url => typeof url === 'string' && url.startsWith('http'));\n  \n  merged[id].all_sources_used.push(...itemSources);\n  \n  // Detect which system this result is from and preserve specific fields\n  if (item.json.source_verdict) {\n    // This is from Source Validator (Skip_Perplexity)\n    merged[id].source_verdict = item.json.source_verdict;\n    merged[id].checked_urls = item.json.checked_urls || [];\n    merged[id].source_explanation = item.json.explanation;\n    console.log(`Claim ${id}: Source Validator = ${item.json.source_verdict}`);\n    \n  } else if (item.json.processing_metadata?.api_source === 'Perplexity') {\n    // This is from Perplexity\n    merged[id].perplexity_verdict = item.json.verdict;\n    merged[id].perplexity_sources = item.json.perplexity_sources || item.json.sources_used || [];\n    merged[id].perplexity_explanation = item.json.explanation;\n    merged[id].perplexity_confidence = item.json.confidence;\n    console.log(`Claim ${id}: Perplexity = ${item.json.verdict}, Sources: ${merged[id].perplexity_sources.length}`);\n    \n  } else if (item.json.processing_metadata?.api_source === 'SerpAPI') {\n    // This is from SerpAPI\n    merged[id].serpapi_verdict = item.json.verdict;\n    merged[id].serpapi_sources = item.json.serpapi_sources || item.json.sources_used || [];\n    merged[id].serpapi_explanation = item.json.explanation;\n    merged[id].serpapi_confidence = item.json.confidence;\n    console.log(`Claim ${id}: SerpAPI = ${item.json.verdict}, Sources: ${merged[id].serpapi_sources.length}`);\n    \n  } else if (item.json.output && typeof item.json.output === 'string') {\n    // This might be unparsed SerpAPI output\n    try {\n      const parsed = JSON.parse(item.json.output);\n      merged[id].serpapi_verdict = parsed.verdict;\n      merged[id].serpapi_sources = Array.isArray(parsed.sources_used) ? parsed.sources_used : \n                                  (typeof parsed.sources_used === 'string' ? [parsed.sources_used] : []);\n      merged[id].serpapi_explanation = parsed.explanation;\n      merged[id].serpapi_confidence = parsed.confidence;\n      console.log(`Claim ${id}: SerpAPI (parsed) = ${parsed.verdict}, Sources: ${merged[id].serpapi_sources.length}`);\n    } catch (e) {\n      console.log(`Failed to parse SerpAPI output for claim ${id}:`, item.json.output);\n    }\n  }\n  \n  // Keep the generic verdicts array for debugging\n  merged[id].verdicts.push({\n    verdict: item.json.verdict || null,\n    explanation: item.json.explanation || null,\n    source_urls: item.json.source_urls || item.json.checked_urls || [],\n    sources_used: item.json.sources_used || [],\n    checker: item.json.source_verdict ? 'source_validator' : \n             item.json.processing_metadata?.api_source?.toLowerCase() || 'unknown'\n  });\n}\n\n// Dedupe all sources for each claim\nObject.values(merged).forEach(entry => {\n  entry.all_sources_used = [...new Set(entry.all_sources_used)];\n  console.log(`Final claim ${entry.claim_id}: ${entry.all_sources_used.length} unique sources`);\n});\n\nconsole.log('=== MERGE RESULTS ===');\nObject.values(merged).forEach(entry => {\n  console.log(`Claim ${entry.claim_id}:`);\n  console.log(`  source_verdict: ${entry.source_verdict}`);\n  console.log(`  perplexity_verdict: ${entry.perplexity_verdict} (${entry.perplexity_sources.length} sources)`);\n  console.log(`  serpapi_verdict: ${entry.serpapi_verdict} (${entry.serpapi_sources.length} sources)`);\n  console.log(`  total_sources: ${entry.all_sources_used.length}`);\n});\n\nreturn Object.values(merged).map(entry => ({ json: entry }));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1000,
        380
      ],
      "id": "00c1e2e7-a60b-4e32-ae22-e917f8f09a6f",
      "name": "Merge_All_Verifications"
    },
    {
      "parameters": {
        "jsCode": "/**\n * Consolidate_Final_Data v2 - WITH SOURCE PRESERVATION\n * =====================\n * Final data preparation before HTML report generation.\n * Ensures every claim has complete, consistent data regardless of processing path.\n */\n\nconst items = $input.all();\nconst originalData = $('Store_Original_Data').all();\n\nconsole.log('=== CONSOLIDATE FINAL DATA ===');\nconsole.log('Input items:', items.length);\nconsole.log('Original data available:', originalData.length);\n\n// Get document title from earlier nodes - try ALL possible sources\nlet documentTitle = 'Unknown Document';\ntry {\n  // Try to get title from multiple workflow nodes\n  const nodesToCheck = [\n    'Add_Metadata',\n    'PDF_To_Text', \n    'Extract_Document_Title',\n    'Text_To_Pages',\n    'Page_Splitter',\n    'Webhook'\n  ];\n  \n  for (const nodeName of nodesToCheck) {\n    try {\n      const nodeData = $(nodeName).all();\n      if (nodeData.length > 0) {\n        const firstItem = nodeData[0];\n        const possibleTitle = firstItem?.json?.Title || \n                             firstItem?.json?.document_title ||\n                             firstItem?.json?.title ||\n                             firstItem?.json?.filename ||\n                             firstItem?.binary?.data0?.fileName;\n        \n        if (possibleTitle && possibleTitle !== 'Unknown Document') {\n          documentTitle = possibleTitle;\n          console.log(`Found title \"${documentTitle}\" from node: ${nodeName}`);\n          break;\n        }\n      }\n    } catch (e) {\n      // Continue to next node\n    }\n  }\n  \n  // If still no title, try from current items\n  if (documentTitle === 'Unknown Document') {\n    for (const item of items) {\n      const possibleTitle = item?.json?.Title || \n                           item?.json?.document_title ||\n                           item?.json?.title;\n      if (possibleTitle && possibleTitle !== 'Unknown Document') {\n        documentTitle = possibleTitle;\n        console.log(`Found title \"${documentTitle}\" from current items`);\n        break;\n      }\n    }\n  }\n  \n  console.log('Final document title:', documentTitle);\n} catch (e) {\n  console.log('Error retrieving document title:', e.message);\n}\n\nreturn items.map((item, index) => {\n  const j = item.json;\n  \n  // Find matching original data\n  const originalItem = originalData.find(orig => orig.json.claim_id === j.claim_id);\n  \n  // Restore core claim data\n  const claim_text = originalItem?.json.original_claim_text || \n                    originalItem?.json.claim_text || \n                    j.claim_text || \n                    'Missing claim text';\n                    \n  const title = documentTitle; // Use the document title we retrieved above\n\n  // Determine final verdict and sources based on processing path\n  let final_verdict, final_sources, explanation, processing_path;\n\n  console.log(`Processing claim ${j.claim_id}:`);\n  console.log(`- source_verdict: ${j.source_verdict}`);\n  console.log(`- perplexity_verdict: ${j.perplexity_verdict}`);\n  console.log(`- serpapi_verdict: ${j.serpapi_verdict}`);\n  console.log(`- final_verdict: ${j.final_verdict}`);\n  console.log(`- perplexity_sources: ${j.perplexity_sources?.length || 0}`);\n  console.log(`- serpapi_sources: ${j.serpapi_sources?.length || 0}`);\n\n  // 1. Check if this went through Source Validator (highest priority)\n  if (j.source_verdict && j.source_verdict === 'Supported') {\n    final_verdict = 'True';\n    final_sources = j.checked_urls || [];\n    explanation = j.explanation || 'Verified via in-document source(s).';\n    processing_path = 'source_validator';\n    \n  } else if (j.source_verdict && j.source_verdict === 'Weakly Supported') {\n    final_verdict = 'Partially True';\n    final_sources = j.checked_urls || [];\n    explanation = j.explanation || 'Partially supported by in-document source(s).';\n    processing_path = 'source_validator';\n    \n  } else if (j.source_verdict && j.source_verdict === 'Refuted') {\n    final_verdict = 'False';\n    final_sources = j.checked_urls || [];\n    explanation = j.explanation || 'Refuted by in-document source(s).';\n    processing_path = 'source_validator';\n    \n  // 2. Check if Claude tiebreaker was used\n  } else if (j.tiebreaker_verdict || j.final_verdict_claude) {\n    final_verdict = j.tiebreaker_verdict || j.final_verdict_claude;\n    final_sources = j.sources_considered || j.tiebreaker_sources || [];\n    explanation = j.tiebreaker_explanation || j.claude_explanation || 'Resolved via expert analysis.';\n    processing_path = 'claude_tiebreaker';\n    \n  // 3. Use the already-determined final verdict (from Final_Verdict_Assembly)\n  } else if (j.final_verdict && j.final_verdict !== 'Unverifiable') {\n    final_verdict = j.final_verdict;\n    final_sources = j.final_sources || j.perplexity_sources || j.serpapi_sources || [];\n    explanation = j.perplexity_explanation || j.serpapi_explanation || 'Verified via web research.';\n    processing_path = j.resolution_method || 'web_research';\n    \n  // 4. Use Perplexity results directly\n  } else if (j.perplexity_verdict && j.perplexity_verdict !== 'Unverifiable') {\n    final_verdict = j.perplexity_verdict;\n    final_sources = j.perplexity_sources || [];\n    explanation = j.perplexity_explanation || 'Verified via web research.';\n    processing_path = 'perplexity';\n    \n  // 5. Use SerpAPI results directly\n  } else if (j.serpapi_verdict && j.serpapi_verdict !== 'Unverifiable') {\n    final_verdict = j.serpapi_verdict;\n    final_sources = j.serpapi_sources || [];\n    explanation = j.serpapi_explanation || 'Verified via search results.';\n    processing_path = 'serpapi';\n    \n  // 6. Fallback\n  } else {\n    final_verdict = 'Unverifiable';\n    final_sources = [];\n    explanation = 'Could not verify claim through available sources.';\n    processing_path = 'unverifiable';\n  }\n\n  // Normalize explanation to string and clean bracketed numbers\n  if (typeof explanation !== 'string') {\n    if (explanation && typeof explanation === 'object') {\n      explanation = JSON.stringify(explanation);\n    } else {\n      explanation = String(explanation || 'No explanation available.');\n    }\n  }\n  \n  // Remove bracketed citation numbers like [3], [4], [5], [3][4][5]\n  explanation = explanation.replace(/\\[\\d+\\]/g, '').trim();\n\n  // Clean and dedupe sources\n  const cleanSources = Array.isArray(final_sources) \n    ? [...new Set(final_sources.filter(url => \n        typeof url === 'string' && url.startsWith('http')\n      ))]\n    : [];\n\n  const result = {\n    json: {\n      // Core claim data\n      claim_id: j.claim_id,\n      claim_text: claim_text,\n      source_paragraph: j.source_paragraph || '',\n      \n      // Final verdict data\n      final_verdict: final_verdict,\n      final_sources: cleanSources,\n      explanation: explanation,\n      \n      // Document metadata\n      Title: title,\n      doc_id: j.doc_id,\n      ingest_ts: j.ingest_ts,\n      \n      // Processing metadata (for debugging)\n      processing_path: processing_path,\n      \n      // PRESERVE ALL SOURCE FIELDS FOR HTML REPORT\n      perplexity_sources: j.perplexity_sources || [],\n      serpapi_sources: j.serpapi_sources || [],\n      checked_urls: j.checked_urls || [],\n      all_sources_used: j.all_sources_used || [],\n      \n      // Keep unsupported sources if any\n      unsupported_urls: j.source_verdict === 'Unsupported' ? (j.checked_urls || []) : [],\n      \n      // Keep all original data for debugging\n      _debug: {\n        source_verdict: j.source_verdict,\n        perplexity_verdict: j.perplexity_verdict,\n        serpapi_verdict: j.serpapi_verdict,\n        tiebreaker_verdict: j.tiebreaker_verdict\n      }\n    }\n  };\n\n  console.log(`Final verdict for claim ${j.claim_id}: ${final_verdict} (${processing_path} path)`);\n  console.log(`Sources: perplexity=${j.perplexity_sources?.length || 0}, serpapi=${j.serpapi_sources?.length || 0}, final=${cleanSources.length}`);\n  \n  return result;\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2100,
        500
      ],
      "id": "644b7716-7706-45e1-a639-bbafe4b05c88",
      "name": "Consolidate_Final_Data"
    },
    {
      "parameters": {
        "mode": "combine",
        "fieldsToMatchString": "claim_id",
        "options": {}
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        380,
        40
      ],
      "id": "f00ac5f4-8ba6-490b-bf7b-7cd3b672fcd1",
      "name": "Merge1"
    },
    {
      "parameters": {
        "jsCode": "/**\n * SerpAPI_Response_Parser\n * Parse the JSON output from SerpAPI_Fact_Agent and normalize the structure\n */\n\nfunction normaliseVerdict(raw) {\n  const v = (raw || '').toString().trim().toLowerCase();\n  if (['true', 'verified', 'supported'].includes(v)) return 'True';\n  if (['false', 'refuted', 'disputed'].includes(v)) return 'False';\n  if (['partially true', 'partial', 'mixed'].includes(v)) return 'Partially True';\n  return 'Unverifiable';\n}\n\nreturn items.map(item => {\n  const rawOutput = item.json.output;\n  let parsed = {};\n  \n  try {\n    parsed = JSON.parse(rawOutput);\n  } catch (e) {\n    console.error('Failed to parse SerpAPI output:', rawOutput);\n    parsed = {\n      claim_id: 'UNKNOWN',\n      verdict: 'Unverifiable',\n      confidence: 0,\n      explanation: 'Failed to parse SerpAPI response',\n      sources_used: []\n    };\n  }\n  \n  // Extract URLs from sources_used field\n  let sources = [];\n  if (parsed.sources_used) {\n    if (typeof parsed.sources_used === 'string') {\n      // Single URL as string\n      sources = [parsed.sources_used];\n    } else if (Array.isArray(parsed.sources_used)) {\n      // Array of URLs\n      sources = parsed.sources_used;\n    }\n  }\n  \n  return {\n    json: {\n      claim_id: parsed.claim_id,\n      verdict: normaliseVerdict(parsed.verdict),\n      confidence: parsed.confidence || 0,\n      explanation: parsed.explanation || 'No explanation provided',\n      sources_used: sources,\n      serpapi_sources: sources,  // Add this specific field\n      processing_metadata: {\n        api_source: 'SerpAPI',\n        received_ts: new Date().toISOString(),\n        error: false\n      }\n    }\n  };\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        880,
        40
      ],
      "id": "4f72d488-ff88-447b-a9e2-c6132f060f56",
      "name": "SerpAPI_Response_Parser"
    },
    {
      "parameters": {
        "jsCode": "// Website Response Handler - Display HTML + Download Option\nconst htmlReport = $input.first().json.report_content;\nconst docTitle = $input.first().json.Title || 'Fact Check Report';\n\n// Create enhanced HTML with download button\nconst enhancedHtml = `\n<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"UTF-8\">\n<title>Fact Check Report - ${docTitle}</title>\n<style>\n.download-header {\nposition: fixed;\ntop: 0;\nleft: 0;\nright: 0;\nbackground: #1a365d;\ncolor: white;\npadding: 10px 20px;\ndisplay: flex;\njustify-content: space-between;\nalign-items: center;\nz-index: 9999;\nbox-shadow: 0 2px 10px rgba(0,0,0,0.1);\n}\n.download-btn {\nbackground: #4a90a4;\ncolor: white;\npadding: 8px 20px;\nborder: none;\nborder-radius: 5px;\ncursor: pointer;\nfont-weight: bold;\ntext-decoration: none;\n}\n.download-btn:hover {\nbackground: #357a8a;\n}\n.report-content {\nmargin-top: 60px;\n}\n</style>\n</head>\n<body>\n<div class=\"download-header\">\n<h3>📊 ${docTitle} - Fact Check Report</h3>\n<button class=\"download-btn\" onclick=\"downloadReport()\">⬇️ Download Report</button>\n</div>\n<div class=\"report-content\">\n${htmlReport}\n</div>\n\n<script>\nfunction downloadReport() {\n// Create a blob with the HTML content\nconst blob = new Blob([\\`${htmlReport.replace(/`/g, '\\\\`')}\\`], { type: 'text/html' });\nconst url = window.URL.createObjectURL(blob);\n\n// Create download link\nconst a = document.createElement('a');\na.href = url;\na.download = '${docTitle.replace(/[^a-z0-9]/gi, '_')}_fact_check_report.html';\ndocument.body.appendChild(a);\na.click();\ndocument.body.removeChild(a);\nwindow.URL.revokeObjectURL(url);\n}\n</script>\n</body>\n</html>\n`;\n\nreturn [{\njson: {\ncontent: enhancedHtml\n}\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2720,
        400
      ],
      "id": "01cd5761-94c8-45e2-ba49-ab1d9663f4ec",
      "name": "Website_Response_Handler"
    },
    {
      "parameters": {
        "respondWith": "text",
        "responseBody": "={{ $json.content }}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Content-Type",
                "value": "text/html"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.3,
      "position": [
        2900,
        400
      ],
      "id": "dc7a00f2-7b78-465f-b34f-0a79da359c7a",
      "name": "Respond to Webhook"
    },
    {
      "parameters": {
        "jsCode": "const pdf = require(\"pdf-parse\");\nconst buffer = Buffer.from(items[0].binary.data0.data, \"base64\");\nconst data = await pdf(buffer);\n\nreturn [{\n  json: {\n    text: data.text\n  },\n  binary: {\n    data0: items[0].binary.data0 // keep binary for downstream compatibility\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -4400,
        340
      ],
      "id": "ebd9f38f-bb9d-4fc9-a6bb-0cfcee961942",
      "name": "PDF_Parse"
    }
  ],
  "pinData": {
    "Tie-Breaker_Agent": [
      {
        "json": {
          "output": "I need to search for more information about Vinod Khosla's political donations to resolve this disagreement.\n\n```json\n{\n  \"claim_id\": \"1\",\n  \"final_verdict\": \"Partially True\",\n  \"final_confidence\": 85,\n  \"tiebreaker_explanation\": \"While Vinod Khosla has predominantly supported Democratic candidates and causes with significant donations, including hosting fundraisers for Obama and Biden, he has also made some contributions to Republican candidates based on their climate policies. His overall donation pattern shows strong Democratic alignment, but not exclusive Democratic support.\",\n  \"decision_factors\": [\"FEC records show majority of donations to Democratic candidates and committees\", \"Has hosted major fundraisers for Democratic presidential candidates including Biden in 2024\", \"Has made some contributions to Republican candidates, particularly those supporting climate initiatives\", \"Self-describes as policy-focused rather than strictly partisan in his political giving\"],\n  \"sources_considered\": [\"https://www.opensecrets.org/donor-lookup/results?name=vinod+khosla\", \"https://www.fec.gov/data/receipts/individual-contributions/?contributor_name=vinod+khosla\"],\n  \"resolution_method\": \"claude_tiebreaker\"\n}\n```"
        }
      },
      {
        "json": {
          "output": "I need to search for evidence about Vinod Khosla's comments regarding Elon Musk and Donald Trump to properly evaluate this claim.\n\nI'll search for: \"Vinod Khosla criticize Elon Musk Trump rhetoric\"\n\n```json\n{\n  \"claim_id\": \"1750126892438-5\",\n  \"final_verdict\": \"Partially True\",\n  \"final_confidence\": 75,\n  \"tiebreaker_explanation\": \"Vinod Khosla has publicly criticized Elon Musk regarding his interactions with Trump, particularly mocking Musk's climate conversation with Trump and questioning Musk's support for Trump. However, Khosla's criticism was more focused on Musk's engagement with Trump and specific policy positions rather than explicitly denouncing Musk for 'amplifying Trump's rhetoric' in those exact terms.\",\n  \"decision_factors\": [\n    \"Khosla did publicly criticize Musk in relation to Trump, including mocking Musk's climate conversation with Trump\",\n    \"Khosla questioned Musk's support for Trump and engaged in public disagreements with Musk on Twitter/X about Trump-related matters\",\n    \"The specific framing that Khosla 'denounced' Musk for 'amplifying Trump's rhetoric' overstates the directness and focus of Khosla's criticism\"\n  ],\n  \"sources_considered\": [\"https://www.cnbc.com/2023/08/07/vinod-khosla-mocks-elon-musk-over-climate-conversation-with-trump.html\", \"https://twitter.com/vkhosla/status/1688259248766763008\"],\n  \"resolution_method\": \"claude_tiebreaker\"\n}\n```"
        }
      }
    ]
  },
  "connections": {
    "Page_Splitter": {
      "main": [
        [
          {
            "node": "Document_URL_Collector",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "PDF_To_Text": {
      "main": [
        []
      ]
    },
    "IF_Page_Router": {
      "main": [
        [
          {
            "node": "Merge_Text_Citations",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Merge_Text_Citations",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge_Text_Citations": {
      "main": [
        [
          {
            "node": "Citations_Dedupe",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Citations_Dedupe": {
      "main": [
        [
          {
            "node": "Footnote_Map_Builder",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity_Helper": {
      "main": [
        [
          {
            "node": "Perplexity_Response_Parser",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity_Response_Parser": {
      "main": [
        [
          {
            "node": "Merge_All_Verifications",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Flatten_Claims": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "Fix_Claims_ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "SerpAPI_Fact_Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Simple Memory": {
      "ai_memory": [
        [
          {
            "node": "SerpAPI_Fact_Agent",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "SerpAPI": {
      "ai_tool": [
        [
          {
            "node": "SerpAPI_Fact_Agent",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    },
    "SerpAPI_Fact_Agent": {
      "main": [
        [
          {
            "node": "SerpAPI_Response_Parser",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Comprehensive_Conflicts_Check": {
      "main": [
        [
          {
            "node": "Conflict_Check",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Conflict_Check": {
      "main": [
        [
          {
            "node": "Tie-Breaker_Agent",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Final_Verdict_Assembly",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Tie-Breaker_Agent": {
      "main": [
        [
          {
            "node": "HTML_Fact_Check_Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Final_Verdict_Assembly": {
      "main": [
        [
          {
            "node": "Consolidate_Final_Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract_from_File": {
      "main": [
        [
          {
            "node": "HTML_Email",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTML_Fact_Check_Report": {
      "main": [
        [
          {
            "node": "Website_Response_Handler",
            "type": "main",
            "index": 0
          },
          {
            "node": "Extract_from_File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTML_Email": {
      "main": [
        [
          {
            "node": "HTTP Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract_Document_Title": {
      "main": [
        [
          {
            "node": "Add_Metadata",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store_Original_Data": {
      "main": [
        [
          {
            "node": "Attach_Supporting_URLs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude": {
      "ai_languageModel": [
        [
          {
            "node": "Tie-Breaker_Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Restore_Claim_Text": {
      "main": [
        [
          {
            "node": "Perplexity_Helper",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Restore_Title": {
      "main": [
        [
          {
            "node": "HTML_Fact_Check_Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Add_Metadata": {
      "main": [
        [
          {
            "node": "Text_To_Pages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claim_Extractor_GPT4o": {
      "main": [
        [
          {
            "node": "Flatten_Claims",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Citation_Parser": {
      "main": [
        [
          {
            "node": "IF_Page_Router",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Footnote_Map_Builder": {
      "main": [
        [
          {
            "node": "Claim_Extractor_GPT4o",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Attach_Supporting_URLs": {
      "main": [
        [
          {
            "node": "Source_Validator_Agent_V2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "IF_Source_Unsupported": {
      "main": [
        [
          {
            "node": "Restore_Claim_Text",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Skip_Perplexity",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Skip_Perplexity": {
      "main": [
        [
          {
            "node": "Merge_All_Verifications",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Document_URL_Collector": {
      "main": [
        [
          {
            "node": "Citation_Parser",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Validate_PDF_Upload": {
      "main": [
        [
          {
            "node": "PDF_Parse",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook": {
      "main": [
        [
          {
            "node": "Move Binary Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Move Binary Data": {
      "main": [
        [
          {
            "node": "Validate_PDF_Upload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract_Hyperlinks_PDF": {
      "main": [
        [
          {
            "node": "Extract_Document_Title",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Text?": {
      "main": [
        [
          {
            "node": "Extract_Hyperlinks_PDF",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Text_To_Pages": {
      "main": [
        [
          {
            "node": "Page_Splitter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Source_Validator_Agent_V2",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Simple Memory1": {
      "ai_memory": [
        [
          {
            "node": "Source_Validator_Agent_V2",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Source_Validator_Agent_V2": {
      "main": [
        [
          {
            "node": "Parse_Validator_JSON",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fix_Claims_ID": {
      "main": [
        [
          {
            "node": "Store_Original_Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SerpAPI1": {
      "ai_tool": [
        [
          {
            "node": "Source_Validator_Agent_V2",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    },
    "Parse_Validator_JSON": {
      "main": [
        [
          {
            "node": "Debug_Verification_Route",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Debug_Verification_Route": {
      "main": [
        [
          {
            "node": "IF_Source_Unsupported",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge_All_Verifications": {
      "main": [
        [
          {
            "node": "Comprehensive_Conflicts_Check",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Consolidate_Final_Data": {
      "main": [
        [
          {
            "node": "Restore_Title",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge1": {
      "main": [
        [
          {
            "node": "SerpAPI_Fact_Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SerpAPI_Response_Parser": {
      "main": [
        [
          {
            "node": "Merge_All_Verifications",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Website_Response_Handler": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "PDF_Parse": {
      "main": [
        [
          {
            "node": "Has Text?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "f1390496-60b5-4b61-b185-1158a4519cc9",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "9aab3be1eed24e0c688da62b1f4b089eb9587a337195142e8a088637e74d0aa2"
  },
  "id": "IRvBv9gSXK7Cbllg",
  "tags": []
}